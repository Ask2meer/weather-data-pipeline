# ğŸŒ¦ï¸ Weather Data Pipeline  
A real-time **data engineering pipeline** built using **Apache Airflow, WeatherStack API, PostgreSQL, DBT, Docker, and Apache Superset**.  
The pipeline automatically **extracts live weather data**, transforms it, loads it into a Postgres database, and visualizes it through interactive dashboards.

---

##  Project Overview
This project demonstrates a **complete end-to-end data pipeline** designed for real-time weather monitoring.  
It automates the entire workflow:

1. **Extract** weather data from WeatherStack API  
2. **Load** the raw data into PostgreSQL  
3. **Transform** the data using **DBT (Data Build Tool)**  
4. **Schedule & orchestrate** the workflow using **Apache Airflow**  
5. **Containerize** everything using Docker  
6. **Visualize** the processed data in **Apache Superset**

This project reflects real-world data engineering concepts such as:  
âœ” ETL pipelines  
âœ” Scheduling & automation  
âœ” Database design  
âœ” Containerization  
âœ” Data transformation  
âœ” Dashboarding  

---


---

##  Technologies Used

| Component | Technology |
|----------|------------|
| Orchestration | Apache Airflow |
| Data Source | WeatherStack API |
| Database | PostgreSQL |
| Transformations | DBT |
| Dashboard | Apache Superset |
| Containerization | Docker & Docker Compose |
| Scripting | Python |

---

##  Project Structure
```

weather-data-project/
â”‚
â”œâ”€â”€ airflow/
â”‚Â  Â â””â”€â”€ dags/
â”‚Â  Â  Â  Â â””â”€â”€ orchestrator.pyÂ  Â  Â  Â # Airflow DAG (The Pipeline Blueprint)
â”œâ”€â”€ api-request/
â”‚Â  Â â”œâ”€â”€ api_request.pyÂ  Â  Â  Â  Â  # Python: Fetches data from WeatherStack API
â”‚Â  Â â””â”€â”€ insert_records.pyÂ  Â  Â  Â # Python: Loads raw data into Postgres
â”œâ”€â”€ dbt/
â”‚Â  Â â””â”€â”€ my_project/Â  Â  Â  Â  Â  Â  Â # DBT Transformation Models (SQL & Jinja)
â”œâ”€â”€ postgres/
â”‚Â  Â â”œâ”€â”€ airflow_init.sqlÂ  Â  Â  Â  Â  # DB setup for Airflow metadata
â”‚Â  Â â””â”€â”€ superset_init.sqlÂ  Â  Â  Â  Â # DB initialization for Superset access
â”œâ”€â”€ docker-compose.yamlÂ  Â  Â  Â  Â # Defines all services (Airflow, Postgres, Superset, etc.)
â””â”€â”€ .env (ignored)              # Secret API Key and other environment variables
```

##  How It Works

### **1. Extract Data**
A Python script (`api_request.py`) calls the **WeatherStack API** and fetches:

- Temperature  
- Humidity  
- Wind speed  
- Local time  
- Weather description  
- Location details  

The data is stored in a JSON structure and passed to Airflow.

---

### **2. Load Data**
Airflow triggers `insert_records.py` to insert raw weather data into PostgreSQL.

---

### **3. Transform Data (DBT)**
DBT performs:

- Data cleaning  
- Creating staging models  
- Aggregation of temperature & humidity  
- Daily weather summary  
- Materialized marts for dashboards  

---

### **4. Visualize Data (Superset)**
Superset connects to PostgreSQL and displays dashboards such as:

- Temperature changes over time  
- Humidity levels  
- Wind speed analysis  
- Summary reports  

---

##  Run the Project

### **Step 1: Clone the Repository**
```bash
git clone https://github.com/Ask2meer/weather-data-pipeline.git
cd weather-data-pipeline
```
### ğŸ‘¤ Author

- Meer Abdullah (Ask2meer)
- Data Engineering & Python Developer
- GitHub: https://github.com/Ask2meer


